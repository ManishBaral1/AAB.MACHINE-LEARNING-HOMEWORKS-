{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -e spambase.data ] || wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'\n",
    "![ -e spambase.names ] || wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(pd.read_csv('spambase.data', header=None))\n",
    "\n",
    "X = data[:,:-1] # features\n",
    "y = data[:,-1] # Last column is label\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def neighbor_means(it):\n",
    "  first = True\n",
    "  for i in it:\n",
    "    if not first:\n",
    "      yield (i + prev) / 2\n",
    "    prev = i\n",
    "    first = False\n",
    "\n",
    "def mode(it):\n",
    "  return Counter(it).most_common(1)[0][0]\n",
    "\n",
    "class Leaf():\n",
    "  def __init__(self, value=None):\n",
    "    self.value = value\n",
    "  \n",
    "  def predict(self, X):\n",
    "    return np.full(X.shape[0], self.value)\n",
    "    \n",
    "  def fit(self, X, y, weights):\n",
    "    self.value = mode(y)\n",
    "\n",
    "class BinaryNode():\n",
    "  def __init__(self, stops, min_leaf_size, min_importance, max_depth):\n",
    "    self.stops = stops\n",
    "    self.min_leaf_size = min_leaf_size\n",
    "    self.min_importance = min_importance\n",
    "    self.max_depth = max_depth\n",
    "\n",
    "    self.feature = None\n",
    "    self.stop = None\n",
    "    self.left = None\n",
    "    self.right = None\n",
    "  \n",
    "  def loss_function(self, y, weights):\n",
    "    if len(y) == 0:\n",
    "      return float('inf')\n",
    "    return ((y != mode(y)) @ weights).mean()\n",
    "\n",
    "  def split_condition(self, X):\n",
    "    return X[:, self.feature] <= self.stop\n",
    "\n",
    "  def importance(self, X, y, weights):\n",
    "    cond = self.split_condition(X)\n",
    "    return self.loss_function(y, weights) - self.loss_function(y[cond], weights[cond]) - self.loss_function(y[~cond], weights[~cond])\n",
    "  \n",
    "  def list_possible(self):\n",
    "    for j, fstops in enumerate(self.stops):\n",
    "      for stop in fstops:\n",
    "        yield j, stop\n",
    "  \n",
    "  def fit(self, X, y, weights):\n",
    "    min_loss = float('inf')\n",
    "    for feature, stop in self.list_possible():\n",
    "      cond = X[:, feature] <= stop\n",
    "      cur_loss = self.loss_function(y[cond], weights[cond]) + self.loss_function(y[~cond], weights[~cond])\n",
    "      if cur_loss <= min_loss:\n",
    "        min_loss = cur_loss\n",
    "        self.feature = feature\n",
    "        self.stop = stop\n",
    "    if self.feature is None:\n",
    "      node = self.make_leaf()\n",
    "      node.fit(X, y, weights)\n",
    "#       print(\"Cannot split\")\n",
    "#       self.__dict__ = node.__dict__  # dark magic\n",
    "      self.predict = node.predict\n",
    "      return\n",
    "    cond = X[:, self.feature] <= self.stop\n",
    "    not_cond = ~cond\n",
    "    self.saved_importance = self.importance(X, y, weights)\n",
    "    self.left = self.create_child(X[cond], y[cond], weights[cond])\n",
    "    self.right = self.create_child(X[not_cond], y[not_cond], weights[not_cond])\n",
    "  \n",
    "  def make_leaf(self):\n",
    "    return Leaf()\n",
    "  \n",
    "  def make_non_leaf(self):\n",
    "    return BinaryNode(self.stops, self.min_leaf_size, self.min_importance, self.max_depth - 1)\n",
    "\n",
    "  def create_child(self, X, y, weights):\n",
    "    if self.max_depth == 0 or len(y) < self.min_leaf_size or self.feature is not None and self.saved_importance < self.min_importance:\n",
    "      child = self.make_leaf()\n",
    "    else:\n",
    "      child = self.make_non_leaf()\n",
    "    child.fit(X, y, weights)\n",
    "    return child\n",
    "\n",
    "  def predict(self, X):\n",
    "    cond = X[:, self.feature] <= self.stop\n",
    "    not_cond = ~cond\n",
    "    y_pred = np.zeros(X.shape[0])\n",
    "    y_pred[cond] = self.left.predict(X[cond])\n",
    "    y_pred[not_cond] = self.right.predict(X[not_cond])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier():\n",
    "  def __init__(self, min_leaf_size=4, min_importance=0.001, max_depth=-1):\n",
    "    self.min_leaf_size = min_leaf_size\n",
    "    self.min_importance = min_importance\n",
    "    self.max_depth = max_depth\n",
    "    \n",
    "  def fit(self, X, y, weights=None):\n",
    "    n_features = X.shape[1]\n",
    "    if weights is None:\n",
    "        weights = np.ones(n_features)\n",
    "    feature_stops = tuple(tuple(neighbor_means(sorted(set(X[:, i])))) for i in range(n_features))\n",
    "    fake = BinaryNode(feature_stops, self.min_leaf_size, self.min_importance, self.max_depth)\n",
    "    self.root = fake.create_child(X, y, weights)\n",
    "\n",
    "  def predict(self, X):\n",
    "    return self.root.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStumpClassifier(DecisionTreeClassifier):\n",
    "    def __init__(self):\n",
    "        DecisionTreeClassifier.__init__(self, 1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier():\n",
    "    def __init__(self, models, positive_class=1, negative_class=0):\n",
    "        self.models = tuple(models)\n",
    "        self.positive_class = positive_class\n",
    "        self.negative_class = negative_class\n",
    "    \n",
    "    def class_to_sign(self, y):\n",
    "        y_sgn = np.ones_like(y)\n",
    "        y_sgn[y == self.negative_class] = -1\n",
    "        return y_sgn\n",
    "    \n",
    "    def sign_to_class(self, y_sgn):\n",
    "        y = np.full_like(y_sgn, self.positive_class)\n",
    "        y[y_sgn < 0] = self.negative_class\n",
    "        return y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.n_samples = n_samples\n",
    "        y_sgn = self.class_to_sign(y)\n",
    "        weights = np.full(n_samples, 1 / n_samples)\n",
    "        self.alphas = []\n",
    "        for m in self.models:\n",
    "            m.fit(X, y, weights=weights)\n",
    "            y_hat = m.predict(X)\n",
    "            e = weights @ (y != y_hat)\n",
    "            alpha = float(np.log((1 - e) / e))\n",
    "            alpha = np.nan_to_num(alpha, True, 0.0, 700.0, -700.0)\n",
    "            self.alphas.append(alpha)\n",
    "            weights *= np.exp(alpha * y_sgn * self.class_to_sign(y_hat))\n",
    "            weights = np.nan_to_num(weights, False, 1.0)\n",
    "            weights /= weights.sum()\n",
    "        self.alphas = np.array(self.alphas)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.sign_to_class(sum((a * self.class_to_sign(m.predict(X)) for a, m in zip(self.alphas, self.models))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from functools import lru_cache\n",
    "\n",
    "class Confusion(namedtuple('Confusion', ['TP', 'FP', 'FN', 'TN'])):\n",
    "    def calculate(y_true, y_pred):\n",
    "        y_true = y_true == True\n",
    "        y_pred = y_pred == True\n",
    "        return Confusion((y_true & y_pred).sum(), (~y_true & y_pred).sum(),\n",
    "                                         (y_true & ~y_pred).sum(), (~y_true & ~y_pred).sum())\n",
    "    \n",
    "    def summary(self):\n",
    "        return f\"\"\"\n",
    "Precision: {self.PPV:.3}\n",
    "Recall: {self.TPR:.3}\n",
    "F1-score: {self.F1:.3}\n",
    "Accuracy: {self.accuracy:.3}\n",
    "\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def P(self):\n",
    "        return self.TP + self.FN\n",
    "    \n",
    "    @property\n",
    "    def N(self):\n",
    "        return self.FP + self.TN\n",
    "    \n",
    "    @property\n",
    "    def PP(self):\n",
    "        return self.TP + self.FP\n",
    "    \n",
    "    @property\n",
    "    def PN(self):\n",
    "        return self.FN + self.TN\n",
    "    \n",
    "    # Precision\n",
    "    @property\n",
    "    @lru_cache()\n",
    "    def PPV(self):\n",
    "        return self.TP / self.PP\n",
    "    \n",
    "    # Recall\n",
    "    @property\n",
    "    @lru_cache()\n",
    "    def TPR(self):\n",
    "        return self.TP / self.P\n",
    "\n",
    "    @property\n",
    "    def F1(self):\n",
    "        return 2 / (1 / self.TPR + 1 / self.PPV)\n",
    "    \n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        return (self.TP + self.TN) / (self.P + self.N)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-cfda11b66637>:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = float(np.log((1 - e) / e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion(TP=246, FP=35, FN=208, TN=662)\n",
      "\n",
      "Precision: 0.875\n",
      "Recall: 0.542\n",
      "F1-score: 0.669\n",
      "Accuracy: 0.789\n",
      "\n",
      "CPU times: user 1min 31s, sys: 3.79 ms, total: 1min 31s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = AdaBoostClassifier(DecisionStumpClassifier() for _ in range(10))\n",
    "model.fit(X_train, y_train)\n",
    "confusion = Confusion.calculate(y_test, model.predict(X_test))\n",
    "print(confusion)\n",
    "print(confusion.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-107-cfda11b66637>:27: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  alpha = float(np.log((1 - e) / e))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion(TP=327, FP=20, FN=127, TN=677)\n",
      "\n",
      "Precision: 0.942\n",
      "Recall: 0.72\n",
      "F1-score: 0.816\n",
      "Accuracy: 0.872\n",
      "\n",
      "CPU times: user 3min 1s, sys: 39 ms, total: 3min 1s\n",
      "Wall time: 3min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model2 = AdaBoostClassifier(DecisionTreeClassifier(1, 0, 2) for _ in range(10))\n",
    "model2.fit(X_train, y_train)\n",
    "confusion2 = Confusion.calculate(y_test, model2.predict(X_test))\n",
    "print(confusion2)\n",
    "print(confusion2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-depth trees gave a better result, but the difference could be less significant with a greater amount of trees.\n",
    "Increasing the depth also increases learning time significantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
